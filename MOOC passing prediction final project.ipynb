{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepik ML contest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final ML contest of the MOOC \"Introduction to Data Science and Machine Learning\" https://stepik.org/course/4852/\n",
    "\n",
    "Having 2 datasets with train data from 2015 to 2018:\n",
    "\n",
    "1) events_train.csv - data about users actions\n",
    "\n",
    "1. step_id - \"step\" can be text lesson, video lesson, page with some info or exercise \n",
    "2. user_id - anonymized user id\n",
    "3. timestamp - action time in unix date format\n",
    "4. action - type of action: \n",
    "    <br>\n",
    "    a  discovered - user clicked on step\n",
    "    <br>\n",
    "    b) viewed - user viewed step\n",
    "    <br>\n",
    "    c) started_attempt - (deprecated) - action before solving an exercise (usually some data downloading)\n",
    "    <br>\n",
    "    d) passed - step is passed (\"next step\" button was clicked)\n",
    "\n",
    "2) submissions_train.csv - data about users submissions\n",
    "\n",
    "1. step_id - \"step\" can be text lesson, video lesson, page with some info or exercise\n",
    "2. timestamp - action time in unix date format\n",
    "3. submission_status - submission status \"correct\" or \"wrong\"\n",
    "4. user_id - anonymized user id\n",
    "\n",
    "We are given 2 test datasets, which include data about the first 2 days of 6184 users activity from 2018 to 2019. \n",
    "\n",
    "1) submission_data_test.csv\n",
    "2) events_data_test.csv\n",
    "\n",
    "We need to predict whether user will complete the course or not. To complete the course is to solve correctly 41+ exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scale_column(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    df[column] = (df[column] - df[column].min())/(df[column].max() - df[column].min())\n",
    "    return df\n",
    "\n",
    "\n",
    "def ensure_data_not_lost(func):\n",
    "    @wraps(func)\n",
    "    def inner(*args, **kwargs):\n",
    "        # contract here - checked df is always first arg (after self)\n",
    "        df = args[1]\n",
    "        size_before = df.size\n",
    "        answer = func(*args, **kwargs)\n",
    "        size_after = answer.size\n",
    "        assert size_before <= size_after, f\"{func.__name__}: Dataframe size decreased!\\\n",
    "                                            Before: {size_before}, after: {size_after}\"\n",
    "        return answer\n",
    "    return inner\n",
    "\n",
    "\n",
    "def check_no_inf_and_nan(columns: list):\n",
    "    def inner_decorator(func):\n",
    "        @wraps(func)\n",
    "        def inner(*args, **kwargs):\n",
    "            answer = func(*args, **kwargs)\n",
    "            for column in columns:\n",
    "                column_to_check = answer[column].replace([np.inf, -np.inf], np.nan)\n",
    "                assert column_to_check.isna().sum() == 0, f\"{func.__name__}, column {column}: \\\n",
    "                                                            found forbidden values!\"\n",
    "            return answer\n",
    "        return inner\n",
    "    return inner_decorator\n",
    "\n",
    "\n",
    "class MOOCMetricsBuilder:\n",
    "    \n",
    "    DAYS_IN_PROGRESS = 30\n",
    "    FIRST_DAYS_CHUNK = 2\n",
    "    COURSE_PASSED_MIN_POINTS = 41\n",
    "    \n",
    "    def __init__(self, events: pd.DataFrame, submissions: pd.DataFrame):\n",
    "        self.events = events\n",
    "        self.submissions = submissions\n",
    "    \n",
    "    def _get_users_actions_time(self, events: pd.DataFrame) -> pd.DataFrame:\n",
    "        first_last_action_time = events.groupby('user_id', as_index=False).agg({'timestamp': [np.min, np.max]})\n",
    "        first_last_action_time.columns = ['user_id', 'min_timestamp', 'max_timestamp']\n",
    "        return first_last_action_time\n",
    "    \n",
    "    def _drop_in_progress_users(self, df: pd.DataFrame, events: pd.DataFrame, \n",
    "                                      submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        last_action = max(events['timestamp'])\n",
    "        finished_threshold = last_action - self.DAYS_IN_PROGRESS * 24 * 60 * 60\n",
    "        finished_users_scores = df[~((df['max_timestamp'] >= finished_threshold) & \\\n",
    "                                     (df['correct_submissions'] < self.COURSE_PASSED_MIN_POINTS))]\n",
    "        finished_users_scores_results = finished_users_scores.assign(\n",
    "            passed_course=finished_users_scores.correct_submissions >= self.COURSE_PASSED_MIN_POINTS)\n",
    "        return finished_users_scores_results\n",
    "    \n",
    "    def _get_correct_score_df(self, events_: pd.DataFrame, submissions_: pd.DataFrame) -> pd.DataFrame:\n",
    "        events_data_ = events_[['user_id']].drop_duplicates()\n",
    "        submissions_data_ = submissions_[['user_id', 'submission_status', 'step_id']]\n",
    "        grouped_correct_submissions_ = submissions_data_[submissions_data_.submission_status == 'correct']\\\n",
    "                                                        .groupby('user_id', as_index=False)['step_id']\\\n",
    "                                                        .nunique().rename({'step_id': 'correct_submissions'}, \n",
    "                                                                          axis=1)\n",
    "        all_correct_submissions_ = events_data_.merge(grouped_correct_submissions_, how='outer')\n",
    "        all_correct_submissions_['correct_submissions'] = all_correct_submissions_['correct_submissions'].fillna(0)\n",
    "        return all_correct_submissions_\n",
    "    \n",
    "    @check_no_inf_and_nan(['correct_submissions'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_correct_submissions(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        grouped_correct_submissions = df[df.submission_status == 'correct']\\\n",
    "                                        .groupby('user_id', as_index=False)['step_id']\\\n",
    "                                        .nunique()\\\n",
    "                                        .rename({'step_id': 'correct_submissions'}, axis=1)\n",
    "        all_correct_submissions = df.merge(grouped_correct_submissions, how='outer')\n",
    "        all_correct_submissions['correct_submissions'] = all_correct_submissions['correct_submissions'].fillna(0)\n",
    "        return all_correct_submissions\n",
    "    \n",
    "    @check_no_inf_and_nan(['wrong_submissions'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_wrong_submissions_df(self, df: pd.DataFrame, submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        wrong_df = submissions[['user_id', 'submission_status']][submissions.submission_status == 'wrong']\\\n",
    "                   .groupby('user_id', as_index=False).count() \\\n",
    "                   .rename({'submission_status': 'wrong_submissions'}, axis=1)\n",
    "        metrics = df.merge(wrong_df, how=\"outer\")\n",
    "        metrics['wrong_submissions'] = metrics['wrong_submissions'].fillna(0)\n",
    "        return metrics\n",
    "    \n",
    "    @check_no_inf_and_nan(['success_rate'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_success_rate(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['success_rate'] = df['correct_submissions']/(df['correct_submissions'] + df['wrong_submissions'])\n",
    "        fill_na = df.success_rate.mean()\n",
    "        fill_minus_inf = df[df.success_rate != -np.inf].success_rate.min()\n",
    "        df['success_rate'] = df['success_rate'].fillna(fill_na)\n",
    "        df['success_rate'] = df['success_rate'].replace(-np.inf, fill_minus_inf)\n",
    "        return df\n",
    "    \n",
    "    @check_no_inf_and_nan(['last_wrong_step_tries'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_last_wrong_step_tries(self, df: pd.DataFrame, submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        users_last_steps_all_tries = submissions.merge(submissions \\\n",
    "                                       .groupby('user_id', as_index=False).max('timestamp'), \n",
    "                                                       on=['user_id', 'step_id']) \\\n",
    "                                       .query('submission_status == \"wrong\"') \\\n",
    "                                       [['step_id', 'user_id']]\n",
    "        last_wrong_step_tries = users_last_steps_all_tries.groupby('user_id', as_index=False)\\\n",
    "                                                          .count()\\\n",
    "                                                          .rename({'step_id': 'last_wrong_step_tries'}, axis=1)\n",
    "        metrics = df.merge(last_wrong_step_tries, how=\"outer\")\n",
    "        fill_na_value = metrics.last_wrong_step_tries.mean()\n",
    "        metrics['last_wrong_step_tries'] = metrics['last_wrong_step_tries'].fillna(fill_na_value)\n",
    "        return metrics\n",
    "    \n",
    "    @check_no_inf_and_nan(['viewed', 'discovered', 'passed'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_events_stats(self,  df: pd.DataFrame, events: pd.DataFrame) -> pd.DataFrame:\n",
    "        user_viewed = events.query('action == \"viewed\"')[['user_id', 'step_id']]\\\n",
    "                      .groupby('user_id', as_index=False).count().rename({'step_id': 'viewed'}, axis=1)\n",
    "        user_discovered = events.query('action == \"discovered\"')[['user_id', 'step_id']]\\\n",
    "                      .groupby('user_id', as_index=False).count().rename({'step_id': 'discovered'}, axis=1)\n",
    "        user_passed = events.query('action == \"passed\"')[['user_id', 'step_id']]\\\n",
    "                      .groupby('user_id', as_index=False).count().rename({'step_id': 'passed'}, axis=1)\n",
    "        \n",
    "        metrics = df.merge(user_viewed, how=\"outer\")\n",
    "        metrics['viewed'] = metrics['viewed'].fillna(0)\n",
    "\n",
    "        metrics = metrics.merge(user_discovered, how=\"outer\")\n",
    "        metrics['discovered'] = metrics['discovered'].fillna(0)\n",
    "\n",
    "        metrics = metrics.merge(user_passed, how=\"outer\")\n",
    "        metrics['passed'] = metrics['passed'].fillna(0)\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    @check_no_inf_and_nan(['viewer_coef'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_viewer_coef(self, metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build new metrics - 'viewer_coef'\n",
    "        This field indicates if user is viewing video lessons while trying to pass course and how many\n",
    "        \"\"\"\n",
    "        metrics['viewer_coef'] = metrics['viewed']/(metrics['correct_submissions'] + metrics['wrong_submissions'])\n",
    "        finite_vals = metrics[~metrics.viewer_coef.isin([np.inf, -np.inf, np.nan])].viewer_coef\n",
    "        fill_na = finite_vals.mean()\n",
    "        fill_plus_inf = finite_vals.max()\n",
    "        fill_minus_inf = finite_vals.min()\n",
    "        metrics['viewer_coef'] = metrics['viewer_coef'].replace(-np.inf, fill_minus_inf)\n",
    "        metrics['viewer_coef'] = metrics['viewer_coef'].replace(np.inf, fill_plus_inf)\n",
    "        metrics['viewer_coef'] = metrics['viewer_coef'].replace(np.nan, fill_na)\n",
    "        return metrics\n",
    "    \n",
    "    @check_no_inf_and_nan(['curiosity_coef'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_curiosity_coef(self, metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build new metrics - 'curiosity_coef'\n",
    "        This field indicates if the user discovered future steps - possible mark of interest\n",
    "        \"\"\"\n",
    "        metrics['curiosity_coef'] = metrics['discovered']/(metrics['passed'] + metrics['viewed'] \n",
    "                                                           + metrics['discovered'])\n",
    "        finite_vals = metrics[~metrics.curiosity_coef.isin([np.inf, -np.inf, np.nan])].curiosity_coef\n",
    "        fill_na = finite_vals.mean()\n",
    "        fill_minus_inf = finite_vals.min()\n",
    "        metrics['curiosity_coef'] = metrics['curiosity_coef'].replace(-np.inf, fill_minus_inf)\n",
    "        metrics['curiosity_coef'] = metrics['curiosity_coef'].replace(np.nan, fill_na)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @check_no_inf_and_nan(['perfectionist_coef', 'all_correct'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_perfectionist_coef(self, metrics: pd.DataFrame, submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build new metrics - 'perfectionist coef'\n",
    "        This field indicates if user not only tried to solve exercise, \n",
    "        but also exepriments with several possible solutions\n",
    "        \"\"\"\n",
    "        all_correct = submissions[['user_id', 'submission_status']].query('submission_status == \"correct\"')\\\n",
    "                                 .groupby('user_id', as_index=False).count()\\\n",
    "                                 .rename({'submission_status': 'all_correct'}, axis=1)\n",
    "        metrics = metrics.merge(all_correct, how=\"outer\")\n",
    "        metrics['all_correct'] = metrics['all_correct'].fillna(0)\n",
    "        metrics['perfectionist_coef'] = metrics['all_correct']/metrics['correct_submissions']\n",
    "        fill_na = metrics.perfectionist_coef.mean()\n",
    "        metrics['perfectionist_coef'] = metrics['perfectionist_coef'].fillna(fill_na)\n",
    "        return metrics\n",
    "    \n",
    "    @check_no_inf_and_nan(['max_wrong_tries'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_max_wrong_tries(self, metrics: pd.DataFrame, submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build new metrics - 'max wrong tries'\n",
    "        This field indicates how stubborn user could be\n",
    "        \"\"\"\n",
    "        max_wrong_tries = submissions.query('submission_status == \"wrong\"')\\\n",
    "                              [['user_id', 'step_id', 'submission_status']]\\\n",
    "                              .groupby(['user_id', 'step_id'], as_index=False).count()\\\n",
    "                              .rename({\"submission_status\": \"max_wrong_tries\"}, axis=1)\\\n",
    "                              [['user_id', 'max_wrong_tries']].groupby('user_id', as_index=False).max()\n",
    "        metrics = metrics.merge(max_wrong_tries, how=\"outer\")\n",
    "        fillna = metrics.max_wrong_tries.mean()\n",
    "        metrics['max_wrong_tries'] = metrics['max_wrong_tries'].fillna(fillna)\n",
    "        return metrics\n",
    "    \n",
    "    @check_no_inf_and_nan(['max_views_one_step'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_max_views_one_step(self, metrics: pd.DataFrame, events: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build new metrics - 'max_views_one_step'\n",
    "        This field indicates how many times can user rewatch a lesson \n",
    "        if there is something not clear for him/her\n",
    "        \"\"\"\n",
    "        max_views_one_step = events.query('action == \"viewed\"')[['user_id', 'step_id', 'action']]\\\n",
    "                             .groupby(['user_id', 'step_id'], as_index=False).count()\\\n",
    "                             .rename({\"action\": \"max_views_one_step\"}, axis=1)[['user_id', 'max_views_one_step']]\\\n",
    "                             .groupby('user_id', as_index=False).max()\n",
    "        metrics = metrics.merge(max_views_one_step, how=\"outer\")\n",
    "        fillna = metrics.max_views_one_step.mean()\n",
    "        metrics['max_views_one_step'] = metrics['max_views_one_step'].fillna(fillna)\n",
    "        return metrics\n",
    "    \n",
    "    @ensure_data_not_lost\n",
    "    def _add_enrolled_date_dayname(self, metrics: pd.DataFrame, events: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Maybe there is difference between users enrolled in course on Monday and the ones enrolled on Saturday\n",
    "        Add day of week to metrics\n",
    "        \"\"\"\n",
    "        first_last_action_time = self._get_users_actions_time(events)\n",
    "        metrics = metrics.merge(first_last_action_time[['user_id', 'min_timestamp']])\n",
    "        metrics['day_name'] = pd.to_datetime(metrics.min_timestamp, unit='s').dt.day_name()\n",
    "        metrics = pd.get_dummies(metrics)\n",
    "        metrics = metrics.drop('min_timestamp', axis=1)\n",
    "        return metrics\n",
    "    \n",
    "    @ensure_data_not_lost\n",
    "    def _add_enrolled_day_hour(self, metrics: pd.DataFrame, events: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Most likely there is a difference between enrolled at 3 AM and at 10 AM\n",
    "        Most likely there is also a difference between users enrolled in the work time and the others\n",
    "        \"\"\"\n",
    "        first_last_action_time = self._get_users_actions_time(events)\n",
    "        metrics = metrics.merge(first_last_action_time[['user_id', 'min_timestamp']])\n",
    "        metrics['start_hour'] = pd.to_datetime(metrics.min_timestamp, unit='s').dt.hour\n",
    "        \n",
    "        # Work time from 9 to 18 inclusively\n",
    "        metrics['work_time'] = ((metrics['start_hour'] // 9) % 2)\n",
    "        metrics = metrics.drop('min_timestamp', axis=1)\n",
    "        metrics['start_hour'] = metrics['start_hour'].astype(str)\n",
    "        metrics = pd.get_dummies(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    @check_no_inf_and_nan(['avg_diff_between_correct_submissions'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_avg_period_between_correct_submissions(self, metrics: pd.DataFrame, \n",
    "                                                    submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        # How fast does the user solve exercises?\n",
    "        sorted_subm = submissions.query('submission_status == \"correct\"')\\\n",
    "                  .groupby(['user_id', 'step_id'], as_index=False).agg({'timestamp': 'min'})\\\n",
    "                  .sort_values(['user_id', 'timestamp'])[['user_id', 'timestamp']]\n",
    "        sorted_subm['prev_diff'] = sorted_subm.groupby('user_id').timestamp.diff()\n",
    "        \n",
    "        user_avg_period = sorted_subm[~sorted_subm.prev_diff.isna()].groupby('user_id', as_index=False)\\\n",
    "                                .agg({'prev_diff': 'sum', 'timestamp': 'count'})\\\n",
    "                                .assign(avg_diff_between_correct_submissions=lambda x: x.prev_diff/x.timestamp)\\\n",
    "                                [['user_id', 'avg_diff_between_correct_submissions']]\n",
    "        metrics = metrics.merge(user_avg_period, how='outer')\n",
    "        fill_na_value = user_avg_period.avg_diff_between_correct_submissions.mean()\n",
    "        metrics['avg_diff_between_correct_submissions'] = metrics['avg_diff_between_correct_submissions']\\\n",
    "                                                          .fillna(fill_na_value)\n",
    "        metrics = scale_column(metrics, 'avg_diff_between_correct_submissions')\n",
    "        return metrics\n",
    "    \n",
    "    @check_no_inf_and_nan(['diff_between_first_action_and_timestamp'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_period_from_first_action_to_first_submission(self, df: pd.DataFrame, events: pd.DataFrame, \n",
    "                                                                submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Maybe the user has enrolled, but that's it, he/she won't solve exercises\n",
    "        first_user_event_data = events[['user_id', 'timestamp']].groupby('user_id', as_index=False)\\\n",
    "                                .min().rename({'timestamp': 'first_event_timestamp'}, axis=1)\n",
    "        first_user_submission_data = submissions[['user_id', 'timestamp']].groupby('user_id', as_index=False)\\\n",
    "                                .min().rename({'timestamp': 'first_submission_timestamp'}, axis=1)\n",
    "        combined_data = first_user_event_data.merge(first_user_submission_data, how=\"outer\")\n",
    "        combined_data['diff_between_first_action_and_timestamp'] = combined_data['first_submission_timestamp'] \\\n",
    "                                                                   - combined_data['first_event_timestamp']\n",
    "        fill_na = combined_data.diff_between_first_action_and_timestamp.mean()\n",
    "        combined_data['diff_between_first_action_and_timestamp'] = combined_data['diff_between_first_action_and_timestamp'].fillna(fill_na)\n",
    "        combined_data = combined_data.drop(['first_submission_timestamp', 'first_event_timestamp'], axis=1)\n",
    "        combined_data = scale_column(combined_data, 'diff_between_first_action_and_timestamp')\n",
    "        df = df.merge(combined_data, how='outer')\n",
    "        return df\n",
    "    \n",
    "    @check_no_inf_and_nan(['diff_between_last_action_and_timestamp'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_period_from_last_submission_to_last_action(self, df: pd.DataFrame, events: pd.DataFrame,\n",
    "                                                              submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Maybe the user doesn't solve exercises for some time, but he/she is still active\n",
    "        last_user_event_data = events[['user_id', 'timestamp']].groupby('user_id', as_index=False)\\\n",
    "                                .max().rename({'timestamp': 'last_event_timestamp'}, axis=1)\n",
    "        last_user_submission_data = submissions[['user_id', 'timestamp']].groupby('user_id', as_index=False)\\\n",
    "                                .max().rename({'timestamp': 'last_submission_timestamp'}, axis=1)\n",
    "        combined_data = last_user_event_data.merge(last_user_submission_data, how=\"outer\")\n",
    "        combined_data['diff_between_last_action_and_timestamp'] = combined_data['last_submission_timestamp'] \\\n",
    "                                                                  - combined_data['last_event_timestamp']\n",
    "        fill_na = combined_data.diff_between_last_action_and_timestamp.mean()\n",
    "        combined_data['diff_between_last_action_and_timestamp'] = combined_data['diff_between_last_action_and_timestamp'].fillna(fill_na)\n",
    "        combined_data = combined_data.drop(['last_submission_timestamp', 'last_event_timestamp'], axis=1)\n",
    "        combined_data = scale_column(combined_data, 'diff_between_last_action_and_timestamp')\n",
    "        df = df.merge(combined_data, how='outer')\n",
    "        return df\n",
    "    \n",
    "    @check_no_inf_and_nan(['success_series_length'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_correct_combo_length(self, df: pd.DataFrame, submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        # How many exercisese was solved correctly in a row (starting from the first one)\n",
    "        submissions_sequence = submissions.merge(submissions.groupby(['user_id', 'step_id'], as_index=False)\\\n",
    "                                                            .agg({'timestamp': 'min'}),\n",
    "                                                 on=['user_id', 'step_id', 'timestamp'])\\\n",
    "                                          .sort_values(['user_id', 'timestamp'])\n",
    "        first_wrong_submission = submissions_sequence.query('submission_status == \"wrong\"')\\\n",
    "                         .groupby('user_id', as_index=False).min('timestamp')\\\n",
    "                         .rename({'timestamp': 'first_wrong_submission_timestamp'}, axis=1)\\\n",
    "                         [['user_id', 'first_wrong_submission_timestamp']]\n",
    "\n",
    "        merged_first_wrong = submissions_sequence.merge(first_wrong_submission, how=\"outer\")\n",
    "        fill_na_value = submissions_sequence.timestamp.max() + 1\n",
    "        merged_first_wrong['first_wrong_submission_timestamp'] = merged_first_wrong['first_wrong_submission_timestamp']\\\n",
    "                                                                 .fillna(fill_na_value)\n",
    "        user_series_lengths = merged_first_wrong.query('timestamp < first_wrong_submission_timestamp')\\\n",
    "                                [['user_id', 'step_id']].groupby('user_id', as_index=False)\\\n",
    "                                .count().rename({\"step_id\": \"success_series_length\"}, axis=1)\n",
    "        df = df.merge(user_series_lengths, how=\"outer\")\n",
    "        df['success_series_length'] = df['success_series_length'].fillna(0)\n",
    "        return df\n",
    "    \n",
    "    @check_no_inf_and_nan(['points'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_difficulty_coef(self, df: pd.DataFrame, submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Different exercises have different complexity\n",
    "        Evaluate the complexity and add this data to the user data\n",
    "        \"\"\"\n",
    "        exercises_rank = submissions.query('submission_status == \"correct\"')\\\n",
    "                                     [['user_id', 'step_id']]\\\n",
    "                                    .drop_duplicates()\\\n",
    "                                    .groupby('step_id', as_index=False).count()\\\n",
    "                                    .rename({'user_id': 'solved_users'}, axis=1)\n",
    "        users_correct_submissions = submissions.query('submission_status == \"correct\"')\\\n",
    "                                                     [['user_id', 'step_id']]\\\n",
    "                                                     .drop_duplicates()\n",
    "        users_points_table = users_correct_submissions.merge(exercises_rank)\n",
    "        users_points_table['solved_users'] = users_points_table['solved_users'].astype(float)\\\n",
    "                                               .apply(lambda x: np.power(x, -1))\n",
    "        users_points_table = users_points_table.rename({'solved_users': 'points'}, axis=1)\n",
    "        user_total_score = users_points_table[['user_id', 'points']].groupby('user_id', as_index=False).sum()\n",
    "        df = df.merge(user_total_score, how=\"outer\")\n",
    "        df['points'] = df['points'].fillna(0)\n",
    "        return df\n",
    "    \n",
    "    @check_no_inf_and_nan(['events_days', 'submissions_days', 'active_hours'])\n",
    "    @ensure_data_not_lost\n",
    "    def _add_activity_data(self, df: pd.DataFrame, events: pd.DataFrame, submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        How many days did the user solve exercises\n",
    "        How many days dif the user watch video lessons etc\n",
    "        How many hours has the user been active\n",
    "        \"\"\"\n",
    "        events['day'] = pd.to_datetime(events.timestamp, unit='s').dt.day\n",
    "        events['hour'] = pd.to_datetime(events.timestamp, unit='s').dt.hour\n",
    "        submissions['day'] = pd.to_datetime(submissions.timestamp, unit='s').dt.day\n",
    "        submissions['hour'] = pd.to_datetime(submissions.timestamp, unit='s').dt.hour\n",
    "        users_events = events[['user_id', 'day', 'hour']].drop_duplicates()\n",
    "        users_submissions = submissions[['user_id', 'day', 'hour']].drop_duplicates()\n",
    "        users_days_events_activity = users_events[['user_id', 'day']].drop_duplicates()\\\n",
    "                                     .groupby('user_id', as_index=False).count()\\\n",
    "                                     .rename({'day': 'events_days'}, axis=1)\n",
    "        users_days_submissions_activity = users_submissions[['user_id', 'day']].drop_duplicates()\\\n",
    "                                     .groupby('user_id', as_index=False).count()\\\n",
    "                                     .rename({'day': 'submissions_days'}, axis=1)\n",
    "        users_active_hours = users_events[['user_id', 'hour']].groupby('user_id', as_index=False).count()\\\n",
    "                                         .rename({'hour': 'active_hours'}, axis=1)\n",
    "        df = df.merge(users_days_events_activity, how='outer')\n",
    "        df = df.merge(users_days_submissions_activity, how='outer')\n",
    "        df['submissions_days'] = df['submissions_days'].fillna(0)\n",
    "        df = df.merge(users_active_hours, how='outer')\n",
    "        return df\n",
    "\n",
    "    def get_first_days_chunk(self, df: pd.DataFrame, events: pd.DataFrame, submissions: pd.DataFrame) -> tuple:\n",
    "        \"\"\"\n",
    "        We are interested only in first days of user activity\n",
    "        Cut the datasets accordingly\n",
    "        \"\"\"\n",
    "        events_merged = events.merge(df, how=\"outer\")\n",
    "        submissions_merged = submissions.merge(df, how=\"outer\")\n",
    "        first_events = events_merged[events_merged.timestamp <= events_merged.min_timestamp \n",
    "                                       + self.FIRST_DAYS_CHUNK * 24 * 60 * 60 ]\n",
    "        first_submissions = submissions_merged[submissions_merged.timestamp <= submissions_merged.min_timestamp \n",
    "                                               + self.FIRST_DAYS_CHUNK * 24 * 60 * 60 ]\n",
    "        return first_events, first_submissions\n",
    "    \n",
    "    def get_finished_users(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Some users haven't passed the course yet, but they are still active \n",
    "        We can't be sure about them, so it's better to delete them\n",
    "        \"\"\"\n",
    "        first_last_action_time = self._get_users_actions_time(self.events)\n",
    "        timed_submissions = self.submissions.merge(first_last_action_time, how=\"outer\")\n",
    "        all_correct_submissions = self._add_correct_submissions(timed_submissions)\n",
    "        finished_users_scores_results = self._drop_in_progress_users(all_correct_submissions, \n",
    "                                                                     self.events, self.submissions)\n",
    "        return finished_users_scores_results\n",
    "    \n",
    "    def build_metrics_df(self, events=None, submissions=None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build our ABT from raw data\n",
    "        \"\"\"\n",
    "        if events is None:\n",
    "            events = self.events\n",
    "        if submissions is None:\n",
    "            submissions = self.submissions\n",
    "\n",
    "        metrics = self._get_correct_score_df(events, submissions)   \n",
    "        metrics = self._add_wrong_submissions_df(metrics, submissions)\n",
    "        metrics = self._add_success_rate(metrics)\n",
    "        metrics = self._add_last_wrong_step_tries(metrics, submissions)\n",
    "        metrics = self._add_events_stats(metrics, events)\n",
    "        metrics = self._add_viewer_coef(metrics)\n",
    "        metrics = self._add_curiosity_coef(metrics)\n",
    "        metrics = self._add_perfectionist_coef(metrics, submissions)\n",
    "        metrics = self._add_max_wrong_tries(metrics, submissions)\n",
    "        metrics = self._add_max_views_one_step(metrics, events)\n",
    "        metrics = self._add_enrolled_date_dayname(metrics, events)\n",
    "        metrics = self._add_enrolled_day_hour(metrics, events)\n",
    "        metrics = self._add_avg_period_between_correct_submissions(metrics, submissions)\n",
    "        metrics = self._add_period_from_first_action_to_first_submission(metrics, events, submissions)\n",
    "        metrics = self._add_period_from_last_submission_to_last_action(metrics, events, submissions)\n",
    "        metrics = self._add_correct_combo_length(metrics, submissions)\n",
    "        metrics = self._add_difficulty_coef(metrics, submissions)\n",
    "        metrics = self._add_activity_data(metrics, events, submissions)\n",
    "        return metrics\n",
    "        \n",
    "    @classmethod\n",
    "    def process_train_data(cls, events: pd.DataFrame, submissions: pd.DataFrame) -> tuple:\n",
    "        metrics_builder = cls(events, submissions)\n",
    "        finished_users = metrics_builder.get_finished_users()\n",
    "        y_train = finished_users[['user_id', 'passed_course']].drop_duplicates()\n",
    "        unique_users = finished_users.drop(['passed_course', \n",
    "                                            'step_id', 'timestamp', \n",
    "                                            'submission_status'], axis=1).drop_duplicates('user_id')\n",
    "        \n",
    "        first_events, first_submissions = metrics_builder.get_first_days_chunk(unique_users, events, submissions)\n",
    "        return metrics_builder.build_metrics_df(first_events, first_submissions), y_train\n",
    "        \n",
    "    @classmethod\n",
    "    def process_test_data(cls, events: pd.DataFrame, submissions: pd.DataFrame) -> pd.DataFrame:\n",
    "        metrics_builder = cls(events, submissions)\n",
    "        return metrics_builder.build_metrics_df()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    \n",
    "    def __init__(self, X_train, y_train, \n",
    "                 X_test=None, y_test=None, \n",
    "                 test_size=0.33, random_state=42, \n",
    "                 scoring='roc_auc',\n",
    "                 scoring_func=lambda fact, pred: roc_auc_score(fact, pred)):\n",
    "        if X_test is None or y_test is None:\n",
    "            X_train_, X_test_, y_train_, y_test_ = train_test_split(X_train, y_train, \n",
    "                                                                    test_size=test_size, \n",
    "                                                                    random_state=random_state,\n",
    "                                                                    stratify=y_train)\n",
    "            self.X_train = X_train_\n",
    "            self.X_test = X_test_\n",
    "            self.y_train = y_train_\n",
    "            self.y_test = y_test_\n",
    "        else:\n",
    "            self.X_train = X_train\n",
    "            self.X_test = X_test\n",
    "            self.y_train = y_train\n",
    "            self.y_test = y_test\n",
    "        \n",
    "        self.scoring = scoring\n",
    "        self.scoring_func = scoring_func\n",
    "        self._classifiers_result_log = []\n",
    "    \n",
    "    def train_randomized(self, params, random_state=42) -> tuple:    \n",
    "        rf = RandomForestClassifier(random_state=random_state)\n",
    "        rscv = RandomizedSearchCV(rf, params, n_jobs=-1, random_state=random_state, scoring=self.scoring)\n",
    "        rscv.fit(self.X_train, self.y_train)\n",
    "        est = rscv.best_estimator_\n",
    "        features = pd.DataFrame({\"features\": list(self.X_train), \"importances\": est.feature_importances_})\n",
    "        return self.scoring_func(self.y_test, est.predict_proba(self.X_test)[:, 1]), est, features.sort_values('importances', ascending=False)\n",
    "\n",
    "    def train_detailed(self, params, random_state=42) -> tuple:\n",
    "        rf = RandomForestClassifier(random_state=random_state)\n",
    "        rscv = GridSearchCV(rf, params, n_jobs=-1, scoring=self.scoring)\n",
    "        rscv.fit(self.X_train, self.y_train)\n",
    "        est = rscv.best_estimator_\n",
    "        features = pd.DataFrame({\"features\": list(self.X_train), \"importances\": est.feature_importances_})\n",
    "        return self.scoring_func(self.y_test, est.predict_proba(self.X_test)[:, 1]), est, features.sort_values('importances', ascending=False)\n",
    "    \n",
    "    def choose_best_classifier(self, total_counts=100, estimates_per_random_state=10, \n",
    "                               accept_score_better_than=0, skip_detailed_training=False):\n",
    "        random_states = [random.randint(1, 1000000) for _ \n",
    "                         in range(math.ceil(total_counts/estimates_per_random_state))]\n",
    "        \n",
    "        params = {\n",
    "                  \"n_estimators\": range(10, 201, 2), \n",
    "                  \"max_depth\": range(5, 50),\n",
    "                  \"min_samples_leaf\": range(10, 60),\n",
    "                  \"min_samples_split\": range(10, 60),\n",
    "                  \"criterion\": [\"gini\", \"entropy\"],\n",
    "                  \"bootstrap\": [True, False]\n",
    "                 }\n",
    "        \n",
    "        print(\"Starting randomized training...\")\n",
    "        already_added = set()\n",
    "        for random_state in random_states:\n",
    "            scores = []\n",
    "            for _ in range(estimates_per_random_state):\n",
    "                score, est, feature_importances = self.train_randomized(params, random_state=random_state)                                                                \n",
    "                if score < accept_score_better_than:\n",
    "                    print(\"Bad score:\", score, \"use next random state...\")\n",
    "                    break\n",
    "\n",
    "                scores.append(score)\n",
    "                if score not in already_added:\n",
    "                    self._classifiers_result_log.append((score, est, feature_importances, random_state))\n",
    "                    already_added.add(score)\n",
    "\n",
    "            if scores:\n",
    "                print(\"Randomized training done for random state:\", random_state, \"with max score\", max(scores))\n",
    "        \n",
    "        score, est, features, random_score = max(self._classifiers_result_log, key=lambda x: x[0])\n",
    "        if skip_detailed_training:\n",
    "            return score, est, features, random_score\n",
    "\n",
    "        print(\"Detailed training...\")\n",
    "        est_range = 5\n",
    "        depth_range = 3\n",
    "        leafs_range = 3\n",
    "        split_range = 3\n",
    "        new_params = {\n",
    "                  \"n_estimators\": range(est.n_estimators - est_range, est.n_estimators + est_range), \n",
    "                  \"max_depth\": range(est.max_depth - depth_range, est.max_depth + depth_range), \n",
    "                  \"min_samples_leaf\": range(est.min_samples_leaf - leafs_range, est.min_samples_leaf + leafs_range),\n",
    "                  \"min_samples_split\": range(est.min_samples_split - split_range, est.min_samples_split + split_range),\n",
    "                  \"criterion\": [\"gini\", \"entropy\"],\n",
    "                  \"bootstrap\": [True, False]\n",
    "        }\n",
    "        new_score, new_est, new_features = self.train_detailed(new_params, random_state=random_score)\n",
    "        print(\"Old score:\", score, \"new score:\", new_score, \"difference:\", new_score - score)\n",
    "        return new_score, new_est, new_features, random_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_similar_users(train_ABT: pd.DataFrame, test_ABT: pd.DataFrame, features: list, key='user_id') -> list:\n",
    "    \"\"\"\n",
    "    Pick from train set similar users to test set and return list with their keys.\n",
    "    Similarity is based on the given list of features.\n",
    "    \n",
    "    <features> - list with features (in order of descending importance)\n",
    "    <key> - name of key field in row\n",
    "    \"\"\"\n",
    "    \n",
    "    assert train_ABT.shape[0], \"Empty train set\"\n",
    "    assert test_ABT.shape[0], \"Empty test set\"\n",
    "    assert features, \"Empty features list\"\n",
    "    assert key, \"Empty row key\"\n",
    "    \n",
    "    work_array = []\n",
    "    TEST_MARK = 0\n",
    "    TRAIN_MARK = 1\n",
    "    for row in test_ABT.itertuples():\n",
    "        row_features = [getattr(row, feature) for feature in features] + [TEST_MARK, getattr(row, key)]\n",
    "        work_array.append(row_features)\n",
    "    for row in train_ABT.itertuples():\n",
    "        row_features = [getattr(row, feature) for feature in features] + [TRAIN_MARK, getattr(row, key)]\n",
    "        work_array.append(row_features)\n",
    "    work_array.sort(reverse=True)\n",
    "    \n",
    "    final_user_ids = []\n",
    "    ready_user_ids = []\n",
    "    test_debt = 0\n",
    "    for row in work_array:\n",
    "        *args, mark, key_value = row\n",
    "        if mark == TRAIN_MARK:\n",
    "            if test_debt > 0:\n",
    "                final_user_ids.append(key_value)\n",
    "                test_debt -= 1\n",
    "            else:\n",
    "                ready_user_ids.append(key_value)\n",
    "        else:\n",
    "            if ready_user_ids:\n",
    "                final_user_ids.append(ready_user_ids.pop())\n",
    "            else:\n",
    "                test_debt += 1\n",
    "    \n",
    "    assert test_ABT.shape[0] == len(final_user_ids), \"Unique pair for some user was not found\"\n",
    "    return final_user_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_events = pd.read_csv('https://stepik.org/media/attachments/course/4852/event_data_train.zip')\n",
    "train_submissions = pd.read_csv('https://stepik.org/media/attachments/course/4852/submissions_data_train.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_events = pd.read_csv('https://stepik.org/media/attachments/course/4852/events_data_test.csv')\n",
    "test_submissions = pd.read_csv('https://stepik.org/media/attachments/course/4852/submission_data_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = MOOCMetricsBuilder.process_train_data(train_events, train_submissions)\n",
    "test_ABT = MOOCMetricsBuilder.process_test_data(test_events, test_submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick users similar to test from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['correct_submissions', 'viewed', 'active_hours', 'wrong_submissions']\n",
    "similar_users_for_train = pick_similar_users(X, test_ABT, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X.sort_values('user_id').drop('user_id', axis=1)\n",
    "y_train = y.sort_values('user_id').drop('user_id', axis=1).T.squeeze()\n",
    "\n",
    "X_val = X[X.user_id.isin(similar_users_for_train)].sort_values('user_id').drop('user_id', axis=1)\n",
    "y_val = y[y.user_id.isin(similar_users_for_train)].sort_values('user_id').drop('user_id', axis=1).T.squeeze()\n",
    "\n",
    "X_train_res = X[~X.user_id.isin(similar_users_for_train)].sort_values('user_id').drop('user_id', axis=1)\n",
    "y_train_res = y[~y.user_id.isin(similar_users_for_train)].sort_values('user_id').drop('user_id', axis=1).T.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our model, validate on picked similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting randomized training...\n",
      "Bad score: 0.8942740137591055 use next random state...\n",
      "Bad score: 0.8797894855216892 use next random state...\n",
      "Bad score: 0.8923922672409855 use next random state...\n",
      "Bad score: 0.8846164451291527 use next random state...\n",
      "Bad score: 0.8922957604880809 use next random state...\n",
      "Randomized training done for random state: 354983 with max score 0.9111697240204226\n",
      "Bad score: 0.8973503354623391 use next random state...\n",
      "Bad score: 0.8934335669950149 use next random state...\n",
      "Bad score: 0.8937487683224289 use next random state...\n",
      "Bad score: 0.8927801865418765 use next random state...\n",
      "Detailed training...\n",
      "Old score: 0.9111697240204226 new score: 0.9112700153518726 difference: 0.00010029133144995317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9112700153518726,\n",
       " RandomForestClassifier(criterion='entropy', max_depth=20, min_samples_leaf=16,\n",
       "                        min_samples_split=41, n_estimators=99,\n",
       "                        random_state=354983),\n",
       "                                    features  importances\n",
       " 0                       correct_submissions     0.117928\n",
       " 9                               all_correct     0.097092\n",
       " 49                                   points     0.094293\n",
       " 6                                    passed     0.083597\n",
       " 5                                discovered     0.067924\n",
       " 4                                    viewed     0.060283\n",
       " 48                    success_series_length     0.051055\n",
       " 52                             active_hours     0.044336\n",
       " 51                         submissions_days     0.044322\n",
       " 8                            curiosity_coef     0.042601\n",
       " 45     avg_diff_between_correct_submissions     0.036020\n",
       " 7                               viewer_coef     0.034677\n",
       " 46  diff_between_first_action_and_timestamp     0.031344\n",
       " 10                       perfectionist_coef     0.027024\n",
       " 47   diff_between_last_action_and_timestamp     0.026092\n",
       " 2                              success_rate     0.024567\n",
       " 1                         wrong_submissions     0.018705\n",
       " 50                              events_days     0.014414\n",
       " 12                       max_views_one_step     0.012640\n",
       " 11                          max_wrong_tries     0.008955\n",
       " 14                          day_name_Monday     0.006941\n",
       " 20                                work_time     0.005234\n",
       " 18                         day_name_Tuesday     0.003919\n",
       " 17                        day_name_Thursday     0.003671\n",
       " 16                          day_name_Sunday     0.003660\n",
       " 19                       day_name_Wednesday     0.003125\n",
       " 13                          day_name_Friday     0.002569\n",
       " 15                        day_name_Saturday     0.002521\n",
       " 27                            start_hour_14     0.002455\n",
       " 35                            start_hour_21     0.002144\n",
       " 24                            start_hour_11     0.001979\n",
       " 23                            start_hour_10     0.001854\n",
       " 36                            start_hour_22     0.001714\n",
       " 32                            start_hour_19     0.001656\n",
       " 25                            start_hour_12     0.001620\n",
       " 29                            start_hour_16     0.001606\n",
       " 44                             start_hour_9     0.001590\n",
       " 26                            start_hour_13     0.001519\n",
       " 31                            start_hour_18     0.001441\n",
       " 42                             start_hour_7     0.001375\n",
       " 30                            start_hour_17     0.001350\n",
       " 43                             start_hour_8     0.001333\n",
       " 28                            start_hour_15     0.001307\n",
       " 34                            start_hour_20     0.001173\n",
       " 41                             start_hour_6     0.001025\n",
       " 40                             start_hour_5     0.000825\n",
       " 39                             start_hour_4     0.000663\n",
       " 3                     last_wrong_step_tries     0.000522\n",
       " 38                             start_hour_3     0.000399\n",
       " 21                             start_hour_0     0.000398\n",
       " 37                            start_hour_23     0.000292\n",
       " 33                             start_hour_2     0.000184\n",
       " 22                             start_hour_1     0.000065,\n",
       " 354983)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify set to test (use the similar ones)\n",
    "predictor = Predictor(X_train, y_train, X_test=X_val, y_test=y_val)\n",
    "threshold = 0.9\n",
    "res = predictor.choose_best_classifier(accept_score_better_than=threshold)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show our model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5455   58]\n",
      " [ 452  219]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.99      0.96      5513\n",
      "        True       0.79      0.33      0.46       671\n",
      "\n",
      "    accuracy                           0.92      6184\n",
      "   macro avg       0.86      0.66      0.71      6184\n",
      "weighted avg       0.91      0.92      0.90      6184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "score, est, features, rs = res\n",
    "pred_y = est.predict(X_val)\n",
    "\n",
    "print(confusion_matrix(y_val, pred_y))\n",
    "print(classification_report(y_val, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = test_ABT['user_id'].sort_values()\n",
    "X_test = test_ABT.sort_values('user_id').drop('user_id', axis=1)\n",
    "\n",
    "predictions = est.predict_proba(X_test)[:, 1]\n",
    "answer = pd.DataFrame({'user_id': users, 'is_gone': predictions})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save result in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = answer.reset_index().drop('index', axis=1)\n",
    "answer.to_csv(\"stepik_answer.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
